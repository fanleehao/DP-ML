# DP & NLP 基础篇（1）



标签（空格分隔）： NLP DP 

---
## 一，基础定义
### 1.1 维基百科
> 深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。
深度学习是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。
深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。表征学习的目标是寻求更好的表示方法并创建更好的模型来从大规模未标记数据中学习这些表示方法。表示方法来自神经科学，并松散地创建在类似神经系统中的信息处理和对通信模式的理解上，如神经编码，试图定义拉动神经元的反应之间的关系以及大脑中的神经元的电活动之间的关系。至今已有数种深度学习框架，如深度神经网络、卷积神经网络和深度置信网络和递归神经网络已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。
另外，“深度学习”已成为类似术语，或者说是神经网络的品牌重塑。

### 1.2 自定义
 - 机器学习大类的一种分支算法，主要是人工神经网络下的
 - 基础是机器学习的分散表示（distributed representation）
 - 主要在语音和图像识别、自然语言处理等应用
 - 深度学习  [ppt 台大][1]
    - 设定人工神经网络 -> 确定学习目标 -> 深度学习  （台大李宏毅）
    - 工具 Keras
    - 选择神经网络 - 评估算法 - 选择最合适的模型
 - CNN 卷积神经网络
 - RNN 递归神经网络
 - 方向：有监督学习、无监督学习、增强学习

### 1.3 深度学习
 - 基础部分
    - 数学：微积分、线性代数、统计学原理、概率论

> 微积分：导数（变化率）、偏导数（f(x)多个变量时对某个变量x的导数即变化率）;求某元偏导数时，其他元均视为常量，即偏导数为某元对函数值的增长率的输出。

**深度学习采用神经网络，解决线性不可分问题**
 
 - 关键词
     - 过拟合&欠拟合：
     - 训练集&测试集：
     - 激活函数：在网络层之间解决非线性控制，本身是非线性的。sigmoid,Relu等   
     - 学习工具：Octave
 - 监督式学习
   对于监督式学习而言，对于所有的数据集，都存在“已知的”算法结果，对某个task进行的结果预测，给出预测的结果概率 p。监督式学习一般又包含了回归问题（即连续值的集合）regression、分类问题（输出结果为离散集合)如聚类，后者通常含多个特征值，可进一步引申到支持向量机。
   监督式学习通常选择sigmond、Relu等
   - 回归问题
     线性回归，采用梯度下降算法来寻求局部最优（可能为多个，凸函数为一个最优）；明确代价函数（cost func）和假设函数（h），学习目的是为了找到线性拟合的最小平方差累和的半平均值。
     多项式线性回归，注意不同特征的取值区间进行归一化；求参数θ的方法可以采用梯度下降法和标准方程（Normal Equation）法，后者不θ=（X‘X）-1X'Y，不需要进行特征值的归一化处理。  一般而言，标准方程只用于线性回归中，且特征值的数量n小于10000以内使用，否则计算量损耗太大。梯度下降法则适用于任何包括非监督学习领域。
   - 过拟合和欠拟合
     过拟合即为高方差（variences）,一般为变量多，无法进行新的样本数据的良好泛化。可以使用较少不必要的特征量或者利用正则化的方法。在正则化中λ偏小，线性拟合中梯度d偏大
     欠拟合即为高偏差（bias）,一般为变量少或样本少。正则化中λ偏大，梯度d偏小
   - 调试和诊断
     - 评价假设：将数据分为3:1:1的训练集、交叉验证集、测试集。利用交叉验证集的误差选择模型，测试集与训练误差作参考
     - 根据学习算法的误差、梯度、λ等维度的不同学习曲线，判断算法的偏差和方差问题，做出相应调整
   - 
 - 非监督式学习
    样本数据无标签，一般用于机器“自主学习”
     - 聚类学习
    聚类分析可用于市场管理、社交网络分析、计算机集群处理分析、以及航天数据的分析。简化而言即为分类聚簇，应用到具体的实际问题.
      - K-menas聚类算法：如何选择K值，K值即为簇的类别和数量。
     - 数据降维问题（PCA）
     在巨量数据的处理如数据压缩、数据可视化工作中，常使用降维处理来简化其复杂程度。主要使用的算法时主成分分析法（PCA）。
     PCA不适用于过拟合的学习情况.
     - 异常检查算法
     利用高斯（正态）分布推导，进行概率分析。异常检测算法适用于非监督学习中，与监督学习不同之处在于，异常点（即正反例的规模）差距更大，不仅仅是标签式相同数据量级别的对比。
     - **专家系统、SVM、核函数算法**
     - 误差分析
       - 一般学习的顺序为：
       ①使用简单的算法实习（关键在于想法）；
       ②绘制学习曲线，调参、特征值、样本数据量大小等；
       ③误差分析和数值评估
       - 误差的度量
       一般闻言，查准率和召回率越高越好





 - 神经网络
    1. 构建大体框架：特征数+标技数；网络层数+每层神经元数（注：初始化参数即权重建议为随机值）
    2. 执行前向传播，求出每个神经元的激活函数值
    3. 编码计算代价函数值
    4. 执行后向传播，计算出神经元的偏导数和误差值
    5. 进行梯度校验，随后停止检验代码
    6. 利用梯度下降等算法找出代价函数的最小值，导出θ


### 1.4 Octave
 - 基础
  - 基本算术运算：直接输入命令行即可，类似Python的CLI
  - 逻辑运算
     - 1 == 2 % 注释
     - 1 ~= 2 % 不等于
     - 1 && 0 基本逻辑运算符
  - 变量 
     输出：直接键入变量名回车或disp(a)
     函数：可返回过个结果
 - 语法特点
    类似MATLAB
     - 常用命令
     > 矩阵： `ones(2,3) % 2x3的单位向量，其他还如zeros(2,3),rand(1,3)`
      帮助：`help ones % 查找指令帮助`
      whos： 查看所有内存区的变量值
      clear： 清除所有内存空间
      save： 保存命令
      矩阵操作：max、sum、prod、floor、ceil、rand、pinv(求逆)
      矩阵：A.*B 点乘； A .^2 幂； 1 ./A 倒数；
      绘图：subplot、Plot；使用hold on 在同一图中可绘制多张图
    
     - 函数
     > 函数保存在外部文件中，可进行路径搜索以导入引用，类似于隐式import
     直接进入当前目录，或使用addpath导入

 - 使用方向

## 二，学习资料
  - 文案资料
   1. [台大电机系李宏毅][2]：
   2. [知乎专栏--刘博][3]
  - 视频类
   1. [Andrew Ng的视频][4]
   2. [台大-机器学习基石-林轩田][5]

## 三，学习路径与方法


  [1]: https://www.slideshare.net/tw_dsconf/ss-62245351?qid=108adce3-2c3d-4758-a830-95d0a57e46bc&v=&b=&from_search=3
  [2]: https://www.slideshare.net/tw_dsconf/ss-62245351?qid=108adce3-2c3d-4758-a830-95d0a57e46bc&v=&b=&from_search=3
  [3]: https://zhuanlan.zhihu.com/p/29527402
  [4]: https://www.coursera.org/learn/machine-learning/lecture
  [5]: https://www.bilibili.com/video/av1624332?from=search&seid=15446150470569228245