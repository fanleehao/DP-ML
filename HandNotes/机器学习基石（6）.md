## 机器学习基石（6）

DP ML

------

### Lecture 7 The  VC Dimension

> 上节课讲述一般化问题，“举一反三”.  主要是利用成长函数的推理，确保Ein和Eout的可能取值相等

### 成长函数的延伸

- 当N和K取值合适时，成长函数的边界可以转化：

  ![1535938376487](D:\SE\DP-ML\HandNotes\assets\1535938376487.png)

- VC Bound证明了在所有假设集合中发生了坏数据假设的几率的值，很小，其值为：![1535938632453](D:\SE\DP-ML\HandNotes\assets\1535938632453.png)

- 如果K≥3且足够大，则

  ![1535938757397](D:\SE\DP-ML\HandNotes\assets\1535938757397.png)

- 机器学习的两个目标——好的假设H以及好的样本数据集D、好的学习算法A能pick一个Ein非常小的g，可以完成某种学习任务：

  ![1535938918841](D:\SE\DP-ML\HandNotes\assets\1535938918841.png)



### **VC维度**

VC维度值最大的非break point的值，即BP点的前一位，例如破发点k，则VC Dimension为k-1. 其标记为d~vc~.

\<center\>VC Dimension of H, d~vc~(H) is largest N for which m~H~(N) = 2^N^\</center\>

- 没有break point时则d~vc~为无限大。d~vc~有限则保证学习有效，假设较好
- VC维度与学习算法A、目标函数f、分类的样本输入的分布p都没有关系，只和整个假设集合相关联。

### 感知学习的VC Dimension（Perceptrons）

  D~vc~ = d + 1 (d 指感知学习中的维度，最简单为1维线性分类、二维平面分类等)

- 多维上的演算

  - 1维：d~vc~ = 2
  - 2D：d~vc~ = 3
  - ...

- 证明过程：d~vc~ ≥ d + 1且d~vc~ ≤ d + 1

- 线性相关

  设a1,a2,...ama1,a2,...am为一组n维向量n维向量，若存在一组不全为0的实数k1,k2,...kmk1,k2,...km，使得 

  \<center\>k1a1+k2a2+k3a3+...+kmam=0k1a1+k2a2+k3a3+...+kmam=0 \</center\>

  则称向量组a1,a2,...,ama1,a2,...,am线性相关，反之，线性无关。

  将向量组写成矩阵，如何通过矩阵的性质判断向量组是线性相关还是线性无关呢？

  - 将矩阵进行初等行变换，化为阶梯型矩阵，若非零行的行数等于向量的个数，即矩阵满秩，则为向量组线性无关；若非零行行数小于向量个数，即矩阵非满秩，则向量组线性相关。
  - 从矩阵的角度来说，如果一个矩阵的行数大于列数，这个矩阵的向量组是线性相关的。

### 自由度

 自由度为假设集合中的参数产生，VC维的意义在机器学习中产生有效的“二元分类”问题的自由度。

- VC维*大体*和自由参数的数量相

- 假设空间集的大小M和VC维

  - 之前曾讲到M和学习的关系，下面引入代换成VC维

    ![1535944525664](D:\SE\DP-ML\HandNotes\assets\1535944525664.png)

  - 

### 模型复杂度和VC维

- ![1535945053161](D:\SE\DP-ML\HandNotes\assets\1535945053161.png)
- 图中的Ω为模型复杂度指数

### 样本复杂度

- 一般样本数量给定10倍的VC维的数据量，而理论上需要10000倍的VC维数据量
-  







