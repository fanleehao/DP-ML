## 机器学习基石（4）

DP ML

---

### LECTURE 5 Training versus Testing

> 回顾：上一课简述机器学习的灵活和适应性。验证了在统计数据的资料数据中抽取足够大的样本，在有限的假设算法中，能够确保训练得出可模拟f的大概率g，且这个g的泛化能力不错——即Ein（g）≈ Eout（g）. 最终我们选择Ein最小的那个.

### 训练和测试

- 机器学习中我们要满足的2个核心问题
  - 能否保证Ein和Eout足够接近——泛化能力
  - 能够保证Ein足够小——误差能力
- 假设数量的集合大小——M值
  - M值偏小，可以保证泛化率，但选择足够小的Ein能力受限
  - M值偏大，可以保证选择够小的Ein，但泛化能力受限

### 数线条

- 无限个假设时，证明用一个有限的值来代替. 

  ![1535446877161](D:\SE\DP-ML\HandNotes\assets\1535959862881.png)

- 从数线条的引申

  二分（Dichotomies）：从样本的假设集合（例如线性分类的线的数量为无限大），映射为一种二分法——即直接从样本的标记值进行分类（例如所有样本XXOO的分类数量），则可映射为2^N.N为样本个数

- 成长函数（Growth Function）

  成长函数有限，上界为2的N次方. m(h) = MAX(H). 最大的为假设的最多种情况。

### 小结

- 成长函数的两面性：多项式较好，指数式较差

  ![1535448453700](D:\SE\DP-ML\HandNotes\assets\1535959917599.png)

- 成长函数中的破发点（Break Point）

  成长函数中随着N的大小变化，第一个与2^N次方不等或开始偏离规律的点。其后的N值越大也都随着上升幅度的放缓.*破发点不一定存在*

  - 破发点会影响成长函数的增长率，随着破发点的出现，成长函数的增长率会逐渐降低